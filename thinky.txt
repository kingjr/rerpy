pandas memory overhead:
created some pickle files containing a full multiindex+colnames data
set, the same as a raw ndarray, and the same with colname
only. Loaded the pickle files one at time into a new python process
and checked RSS in top:
  python -c 'import cPickle; import resource; d = cPickle.load(open("df-colnames-only.pickle", "rb")); import time; time.sleep(100)'
results:
  ndarray: 253 megabytes
  colnames: 265 megabytes
  multiindex: 416 megabytes
    with pandas 0.8: 296. Okay, that's more like it.
df is 981504 x 32. So the theoretical cost for row indexes is:
  32 bit integer: 3.7 MiB
  64 bit integer: 7.5 MiB



have a special "cals" command that (1) arranges for the given events
to be included in a special bin in all output files, (2) eliminates
the given events from consideration for other stuff (maybe even just
deletes their code and condition entries)

or another way to do this, scoped restrictions?
select <query>
  # indented block
  # <query> is automatically ANDed with any queries within this
  # section

define "groups" as short-hands for certain queries? maybe better to
just add fields.

match code == 12 and condition == 0 as BASE
next with code == 13 within 200-800


wacky idea, just make the input file python code?
  (py2 or py3?)


importing events from eeglab: this page says how to import events
*into* eeglab, and has sample files to do this with, plus points to
the function that lets you export data to csv... so it should have the
right leads for figuring out how this information is stored in the
eeglab structure:
http://sccn.ucsd.edu/wiki/A02:_Importing_Event_Epoch_Info
